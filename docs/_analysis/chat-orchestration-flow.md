# Chat-Orchestrierungs-Flow: Vereinfachte Dokumentation

## Ãœbersicht

Dieses Dokument beschreibt den vereinfachten Flow der Chat-Orchestrierung von der Frage bis zur Antwort. Die Logik wurde stark vereinfacht: TOC-Queries verwenden Summary oder ChunkSummary basierend auf Token-Budget, normale Fragen verwenden immer RAG (chunk) mit Warnung bei zu wenig relevanten Dokumenten.

## Flow-Diagramm

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   1. Request empfangen                          â”‚
â”‚              (POST /api/chat/[libraryId]/stream)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   2. Authentifizierung &                         â”‚
â”‚                      Library-Context laden                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   3. Query-Typ bestimmen                        â”‚
â”‚              â€¢ isTOCQuery? (TOC_QUESTION)                        â”‚
â”‚              â€¢ Normale Frage                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                       â”‚
                â–¼                                                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   TOC-Query       â”‚   â”‚   Normale Frage      â”‚
    â”‚   (Story Mode)    â”‚   â”‚   (immer RAG)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                        â”‚
              â”‚                        â”‚
              â”‚                        â”‚
              â”‚                        â”‚
              â”‚                        â”‚
              â”‚                        â”‚
              â”‚                        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  4. Retriever-Entscheidung           â”‚
        â”‚  TOC: summary oder chunkSummary      â”‚
        â”‚  (basierend auf Token-Budget)        â”‚
        â”‚  Normale Frage: chunk (RAG)          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           â”‚           â”‚
        â–¼           â–¼           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Summary â”‚ â”‚ChunkSumm â”‚ â”‚ Chunk  â”‚
    â”‚(TOC)   â”‚ â”‚(TOC)     â”‚ â”‚(RAG)   â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚           â”‚           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  5. Retrieval          â”‚
        â”‚  (Quellen abrufen)    â”‚
        â”‚  â€¢ RAG: Score-PrÃ¼fung â”‚
        â”‚  â€¢ Warnung bei < 0.7  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  6. Prompt-Building   â”‚
        â”‚  â€¢ TOC-Prompt         â”‚
        â”‚  â€¢ Normal-Prompt      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  7. LLM-Aufruf        â”‚
        â”‚  (OpenAI API)         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  8. Response-Parsing  â”‚
        â”‚  â€¢ StoryTopicsData    â”‚
        â”‚  â€¢ Normal-Answer      â”‚
        â”‚  â€¢ Warnung hinzufÃ¼gen â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  9. Complete          â”‚
        â”‚  (Stream beenden)     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Detaillierte Fallbeschreibungen

### Fall 1: TOC-Query (Story Mode)

**Trigger**: `isTOCQuery === true` (Frage entspricht `TOC_QUESTION` ODER `asTOC === true`)

**Flow**:
1. âœ… **Retriever-Entscheidung**: Basierend auf Token-Budget
   - Berechnet: `estimatedTokens = totalChunks Ã— CHAT_AVG_TOKENS_PER_CHUNK`
   - Wenn `estimatedTokens >= CHAT_MAX_INPUT_TOKENS`: `summary` (MongoDB Summaries)
   - Wenn `estimatedTokens < CHAT_MAX_INPUT_TOKENS`: `chunkSummary` (alle Chunks)
2. âœ… **Retrieval**: 
   - `summary`: `summariesMongoRetriever` - LÃ¤dt alle MongoDB-Dokument-Summaries
   - `chunkSummary`: `chunkSummaryRetriever` - LÃ¤dt alle Chunks der gefilterten Dokumente
3. âœ… **Prompt**: `buildTOCPrompt()` - Spezieller Prompt fÃ¼r ThemenÃ¼bersicht
4. âœ… **LLM**: Generiert strukturierte ThemenÃ¼bersicht
5. âœ… **Parsing**: `parseStoryTopicsData()` - Extrahiert strukturierte Daten
6. âœ… **Output**: `StoryTopicsData` + Markdown-Answer (fÃ¼r RÃ¼ckwÃ¤rtskompatibilitÃ¤t)

**Vorteile**:
- Automatische Entscheidung basierend auf Token-Budget
- Bei wenigen Dokumenten: PrÃ¤ziser (chunkSummary)
- Bei vielen Dokumenten: Ãœbersichtlich (summary)

**Nachteile**:
- AbhÃ¤ngig von QualitÃ¤t der Summaries (bei summary)

---

### Fall 2: Normale Frage - Summary-Modus (nur explizit)

**Trigger**: 
- Explizit: `retriever=summary` oder `retriever=doc`

**Flow**:
1. âœ… **Retriever-Entscheidung**: `summary` (nur wenn explizit gesetzt)
2. âœ… **Retrieval**: `summariesMongoRetriever`
   - LÃ¤dt alle MongoDB-Dokument-Summaries (gefiltert)
   - Keine Embedding-Suche
3. âœ… **Prompt**: `buildPrompt()` - Normaler Prompt mit Summaries
4. âœ… **LLM**: Generiert Antwort basierend auf Summaries
5. âœ… **Parsing**: `parseStructuredLLMResponse()` - Standard-Parsing

**Hinweis**: Dieser Modus wird nur noch verwendet, wenn explizit gewÃ¤hlt. Automatisch wird immer RAG verwendet.

---

### Fall 3: TOC-Query - ChunkSummary-Modus

**Trigger**: 
- TOC-Query UND `estimatedTokens < CHAT_MAX_INPUT_TOKENS`

**Entscheidungskriterien**:
```typescript
estimatedTokens = totalChunks Ã— CHAT_AVG_TOKENS_PER_CHUNK
if (estimatedTokens < CHAT_MAX_INPUT_TOKENS) {
  mode = 'chunkSummary' // FÃ¼r TOC
}
```

**Flow**:
1. âœ… **Retriever-Entscheidung**: `chunkSummary` (nur fÃ¼r TOC)
   - Automatisch basierend auf Token-Budget
   - Berechnet: `sumChunkCounts()` aus MongoDB
2. âœ… **Retrieval**: `chunkSummaryRetriever`
   - Schritt 1: LÃ¤dt gefilterte Dokumente aus MongoDB (nur `fileIds`)
   - Schritt 2: LÃ¤dt **alle** Chunks dieser Dokumente aus MongoDB
   - **OHNE Embedding-Suche** (direkter Filter: `fileId: { $in: [...] }`)
   - Filter-Struktur: `{ kind: 'chunk', libraryId, user, fileId: { $in: [...] } }`
3. âœ… **Prompt**: `buildTOCPrompt()` - TOC-Prompt mit allen Chunks
4. âœ… **LLM**: Generiert strukturierte ThemenÃ¼bersicht
5. âœ… **Parsing**: `parseStoryTopicsData()` - Extrahiert strukturierte Daten

**Vorteile**:
- VollstÃ¤ndiger Kontext (alle Chunks)
- PrÃ¤zisere ThemenÃ¼bersicht als Summary-Modus
- Keine Embedding-Suche nÃ¶tig

**Hinweis**: Dieser Modus wird nur fÃ¼r TOC-Queries verwendet, nicht fÃ¼r normale Fragen.

---

### Fall 4: Normale Frage - Chunk-Modus (RAG)

**Trigger**: 
- Immer fÃ¼r normale Fragen (automatisch)
- Explizit: `retriever=chunk`

**Flow**:
1. âœ… **Retriever-Entscheidung**: `chunk` (RAG) - Immer fÃ¼r normale Fragen
2. âœ… **Retrieval**: `chunksRetriever`
   - Schritt 1: Embedding der Frage generieren
   - Schritt 2: Semantische Suche in MongoDB Vector Search (Vector-Similarity)
   - Schritt 3: Top-K relevante Chunks zurÃ¼ckgeben
   - Optional: Chapter-Summaries zusÃ¤tzlich abrufen
   - **Warnung**: Wenn alle Scores < 0.7 â†’ Warnung generieren
3. âœ… **Prompt**: `buildPrompt()` - Normaler Prompt mit relevanten Chunks
4. âœ… **LLM**: Generiert Antwort basierend auf relevanten Chunks
5. âœ… **Parsing**: `parseStructuredLLMResponse()` - Standard-Parsing
6. âœ… **Warnung**: Falls vorhanden, wird zur Antwort hinzugefÃ¼gt

**Vorteile**:
- Semantisch relevante Chunks (beste QualitÃ¤t)
- Funktioniert auch bei vielen Dokumenten
- Flexibel (Top-K kann angepasst werden)
- Warnung bei zu wenig relevanten Dokumenten

**Nachteile**:
- Langsamer (Embedding-Generierung + Vektor-Suche)
- Teurer (mehr API-Calls)

---

## Entscheidungslogik: `decideRetrieverMode()`

### PrioritÃ¤ten (in Reihenfolge):

1. **TOC-Query**: Entscheidung basierend auf Token-Budget
   ```typescript
   if (isTOCQuery) {
     totalChunks = sumChunkCounts(filter)
     estimatedTokens = totalChunks Ã— CHAT_AVG_TOKENS_PER_CHUNK
     
     if (estimatedTokens < CHAT_MAX_INPUT_TOKENS) {
       return { mode: 'chunkSummary' } // PrÃ¤ziser bei wenigen Dokumenten
     } else {
       return { mode: 'summary' } // Ãœbersichtlich bei vielen Dokumenten
     }
   }
   ```

2. **Explizite Auswahl**: Respektiere User-Input
   ```typescript
   if (explicitRetriever && explicitRetriever !== 'auto') {
     // chunkSummary nur fÃ¼r TOC verfÃ¼gbar
     return { mode: explicitRetriever === 'doc' ? 'summary' : explicitRetriever }
   }
   ```

3. **Normale Fragen**: Immer RAG
   ```typescript
   return { mode: 'chunk' } // Immer RAG fÃ¼r normale Fragen
   ```

### Konfiguration:

- `CHAT_MAX_INPUT_TOKENS`: Token-Budget (Standard: aus `budget.ts`)
- `CHAT_AVG_TOKENS_PER_CHUNK`: Durchschnittliche Token pro Chunk (Standard: 300)
- `RAG_MIN_SCORE_THRESHOLD`: Mindest-Score fÃ¼r relevante Dokumente (Standard: 0.7)

---

## Warnungs-Logik

### Warnung bei zu wenig relevanten Dokumenten

**Trigger**: 
- RAG-Modus (chunk)
- Alle gefundenen Dokumente haben Score < 0.7

**Warnung**:
```
"Die zugrundeliegenden Dokumente enthalten zu wenig passenden Inhalt. 
Bitte formulieren Sie die Frage um oder erweitern Sie die Anzahl der 
zugrundeliegenden Dokumente (Facettenfilter anpassen)."
```

**Implementierung**:
- PrÃ¼fung nach `queryVectors()` in `chunksRetriever`
- Warnung wird in `RetrieverOutput.warning` zurÃ¼ckgegeben
- Im Orchestrator wird Warnung zur Antwort hinzugefÃ¼gt

---

## Checkbox fÃ¼r ThemenÃ¼bersicht

### Feature: "Als ThemenÃ¼bersicht anzeigen"

**Beschreibung**: 
- Checkbox im Chat-Input fÃ¼r normale Fragen
- Wenn aktiviert: Antwort wird als ThemenÃ¼bersicht (StoryTopicsData) formatiert
- ErmÃ¶glicht es Benutzern, normale Fragen als strukturierte ThemenÃ¼bersicht zu erhalten

**Implementierung**:
- Parameter `asTOC` im Request-Body
- Wenn `asTOC === true`: `isTOCQuery` wird auf `true` gesetzt
- Verwendet TOC-Prompt und StoryTopicsData-Parsing

---

## Implementierte Ã„nderungen

### 1. Vereinfachte TOC-Logik

- TOC verwendet `summary` oder `chunkSummary` basierend auf Token-Budget
- `chunkSummary` nur fÃ¼r TOC verfÃ¼gbar (nicht fÃ¼r normale Fragen)

### 2. Normale Fragen: Immer RAG

- Automatisch immer `chunk` (RAG) fÃ¼r normale Fragen
- Keine Token-Budget-PrÃ¼fung mehr fÃ¼r normale Fragen
- Warnung bei zu wenig relevanten Dokumenten (Score < 0.7)

### 3. Frage-Analyse entfernt

- Keine LLM-basierte Frage-Analyse mehr
- Chat-Title wird direkt aus Frage generiert (erste 60 Zeichen)
- Vereinfachter Flow ohne zusÃ¤tzliche LLM-Calls

### 4. Checkbox fÃ¼r ThemenÃ¼bersicht

- Checkbox "Als ThemenÃ¼bersicht anzeigen" im Chat-Input
- ErmÃ¶glicht normale Fragen als ThemenÃ¼bersicht zu formatieren

---

## Zusammenfassung: Vereinfachte Logik

### Implementiert:

```
TOC-Query:
  Wenn estimatedTokens < CHAT_MAX_INPUT_TOKENS â†’ chunkSummary (prÃ¤ziser)
  Wenn estimatedTokens >= CHAT_MAX_INPUT_TOKENS â†’ summary (Ã¼bersichtlich)

Normale Frage:
  Immer â†’ chunk (RAG)
  Warnung wenn alle Scores < 0.7

Checkbox "Als ThemenÃ¼bersicht":
  Setzt isTOCQuery = true â†’ verwendet TOC-Logik
```

---

## Implementierungsstatus

1. âœ… TOC-Logik vereinfacht (Token-Budget basiert)
2. âœ… Normale Fragen: Immer RAG
3. âœ… Warnung bei zu wenig relevanten Dokumenten
4. âœ… Frage-Analyse entfernt
5. âœ… Checkbox fÃ¼r ThemenÃ¼bersicht hinzugefÃ¼gt
6. âœ… Dokumentation aktualisiert

## NÃ¤chste Schritte (optional)

1. âš ï¸ Tests fÃ¼r Edge-Cases (wenige Dokumente, viele Dokumente)
2. âš ï¸ Monitoring: Welcher Modus wird wann gewÃ¤hlt?
3. âš ï¸ Top-K Erweiterung evaluieren (bei zu wenig relevanten Dokumenten)

---

## Performance-Analyse: Timing-Daten

### Gemessene Timings (Beispiel aus Produktion)

**RAG-Flow (chunk-Modus) - Normale Frage:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Schritt                    â”‚ Timing    â”‚ Anteil â”‚ Status   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Embedding-Generierung   â”‚ ~514ms    â”‚ 11%    â”‚ âœ… OK    â”‚
â”‚ 2. Vector-Suche (Query)    â”‚ ~1187ms   â”‚ 26%    â”‚ âš ï¸ Langsamâ”‚
â”‚ 3. Fetch Neighbors          â”‚ ~2902ms   â”‚ 64%    â”‚ âŒ Sehr  â”‚
â”‚                            â”‚           â”‚        â”‚   langsam â”‚
â”‚ 4. Prompt-Building         â”‚ ~50-100ms â”‚ 1-2%   â”‚ âœ… OK    â”‚
â”‚ 5. LLM-Aufruf             â”‚ ~2000-5000msâ”‚ 44-55%â”‚ âš ï¸ Variabelâ”‚
â”‚                            â”‚           â”‚        â”‚           â”‚
â”‚ TOTAL                      â”‚ ~4600-8700msâ”‚ 100% â”‚ âš ï¸        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detaillierte Schritt-Analyse

#### 1. Embedding-Generierung (`embed`)
- **Timing**: ~514ms
- **Was passiert**: Frage wird in Embedding-Vektor umgewandelt
- **API-Call**: OpenAI Embeddings API
- **Bewertung**: âœ… Akzeptabel (~500ms ist normal fÃ¼r Embeddings)
- **Optimierungspotenzial**: Gering (abhÃ¤ngig von OpenAI)

#### 2. Vector-Suche (`query`)
- **Timing**: ~1187ms
- **Was passiert**: 
  - Semantische Suche in MongoDB Vector Search
  - Top-K relevante Chunks finden (Standard: 20)
  - Parallel: Chapter-Summaries suchen (Top-10)
- **API-Call**: MongoDB `$vectorSearch` Aggregation (2 parallel)
- **Bewertung**: âš ï¸ Langsam (kÃ¶nnte optimiert werden)
- **Optimierungspotenzial**: 
  - Parallelisierung bereits implementiert âœ…
  - Top-K kÃ¶nnte reduziert werden (wenn nicht alle benÃ¶tigt)
  - MongoDB Vector Search Index-Performance prÃ¼fen

#### 3. Fetch Neighbors (`fetchNeighbors`)
- **Timing**: Variabel (abhÃ¤ngig von Anzahl der Nachbarn)
- **Was passiert**:
  - Window-basierte Nachbar-Chunks abrufen
  - Beispiel: Top-20 Chunks â†’ Window Â±1-3 â†’ ~60-74 IDs
  - Direkte MongoDB-Abfrage mit `_id: { $in: [...] }`
- **API-Call**: MongoDB `find()` Query (1 Request mit vielen IDs)
- **Bewertung**: âœ… Schneller als Pinecone Fetch API
- **Vorteile**: 
  - Direkte MongoDB-Abfrage ist effizienter
  - Alle Metadaten bereits verfÃ¼gbar (keine separate Fetch nÃ¶tig)
  - Window-GrÃ¶ÃŸe (`windowByLength`) bestimmt Anzahl der IDs
- **Optimierungspotenzial**: 
  - **Mittel**: Window-GrÃ¶ÃŸe dynamisch anpassen
  - **Niedrig**: Optional machen (nur wenn wirklich nÃ¶tig)

#### 4. Prompt-Building
- **Timing**: ~50-100ms (geschÃ¤tzt)
- **Was passiert**: Prompt aus Quellen zusammenbauen
- **Bewertung**: âœ… OK (lokale Operation)
- **Optimierungspotenzial**: Gering

#### 5. LLM-Aufruf
- **Timing**: ~2000-5000ms (variabel)
- **Was passiert**: OpenAI API-Call fÃ¼r Antwort-Generierung
- **Bewertung**: âš ï¸ Variabel (abhÃ¤ngig von AntwortlÃ¤nge, Modell, Last)
- **Optimierungspotenzial**: 
  - Modell-Auswahl (gpt-4.1-mini ist schneller als gpt-4o)
  - Streaming bereits implementiert âœ…
  - Token-Budget-Management bereits implementiert âœ…

### Identifizierte Performance-Probleme

#### ğŸ”´ Problem 1: Fetch Neighbors ist zu langsam
- **Impact**: Hoch (64% der Retrieval-Zeit)
- **Ursache**: Ein Request mit vielen IDs (74+)
- **LÃ¶sung**: 
  1. Batch-GrÃ¶ÃŸe reduzieren (z.B. max 20 IDs pro Request)
  2. Parallelisierung (mehrere Requests parallel)
  3. Window-GrÃ¶ÃŸe dynamisch anpassen (kleineres Window bei vielen Chunks)
  4. Optional machen (nur wenn wirklich nÃ¶tig)

#### ğŸŸ¡ Problem 2: Vector-Suche kÃ¶nnte schneller sein
- **Impact**: Mittel (26% der Retrieval-Zeit)
- **Ursache**: MongoDB Vector Search Query-Performance
- **LÃ¶sung**: 
  1. Top-K reduzieren (wenn nicht alle benÃ¶tigt)
  2. Vector Search Index-Performance prÃ¼fen
  3. Caching fÃ¼r Ã¤hnliche Queries

#### ğŸŸ¢ Problem 3: Embedding-Generierung ist akzeptabel
- **Impact**: Niedrig (11% der Retrieval-Zeit)
- **Status**: âœ… OK

### OptimierungsvorschlÃ¤ge

#### PrioritÃ¤t 1: Fetch Neighbors optimieren (Hoch)

**Option A: Batch-GrÃ¶ÃŸe reduzieren**
```typescript
// Aktuell: Ein Request mit allen IDs
const fetched = await fetchVectors(idx.host, apiKey, ids) // 74 IDs

// Optimiert: Mehrere Requests mit kleineren Batches
const BATCH_SIZE = 20
const batches = []
for (let i = 0; i < ids.length; i += BATCH_SIZE) {
  batches.push(ids.slice(i, i + BATCH_SIZE))
}
const fetched = await Promise.all(
  batches.map(batch => fetchVectors(idx.host, apiKey, batch))
)
```

**Option B: Window-GrÃ¶ÃŸe dynamisch anpassen**
```typescript
// Aktuell: Feste Window-GrÃ¶ÃŸe basierend auf answerLength
const windowByLength = input.answerLength === 'ausfÃ¼hrlich' ? 3 : ...

// Optimiert: Dynamisch basierend auf Anzahl der Matches
const windowSize = matches.length > 30 ? 1 : matches.length > 15 ? 2 : 3
```

**Option C: Optional machen**
```typescript
// Nur wenn wirklich nÃ¶tig (z.B. wenn Chunks sequenziell sind)
const needsNeighbors = matches.some(m => {
  const { base, chunk } = parseId(m.id)
  return Number.isFinite(chunk)
})
if (needsNeighbors) {
  // Fetch neighbors
} else {
  // Verwende Original-Matches direkt
}
```

#### PrioritÃ¤t 2: Vector-Suche optimieren (Mittel)

**Option A: Top-K reduzieren**
```typescript
// Aktuell: baseTopK = 20
// Optimiert: Dynamisch basierend auf Budget
const baseTopK = budget > 50000 ? 30 : budget > 30000 ? 20 : 15
```

**Option B: Caching fÃ¼r Ã¤hnliche Queries**
```typescript
// Cache fÃ¼r Embeddings Ã¤hnlicher Fragen (z.B. innerhalb von 5 Minuten)
const cacheKey = hashQuestion(input.question)
const cached = await getCachedEmbedding(cacheKey)
if (cached) {
  // Verwende cached embedding
}
```

#### PrioritÃ¤t 3: Monitoring verbessern (Niedrig)

- Detaillierte Timing-Logs fÃ¼r jeden Schritt
- Alerts bei langsamen Queries (>5s)
- Dashboard fÃ¼r Performance-Metriken

---

## Flow-Validierung: Aktueller Stand

### âœ… Validierung: Flow stimmt noch

Der dokumentierte Flow entspricht der aktuellen Implementierung:

1. âœ… **TOC-Query-Logik**: 
   - Token-Budget-basierte Entscheidung zwischen `summary` und `chunkSummary`
   - Implementiert in `decideRetrieverMode()`

2. âœ… **Normale Fragen**: 
   - Immer RAG (`chunk`-Modus)
   - Warnung bei Score < 0.7
   - Implementiert in `chunksRetriever`

3. âœ… **Retrieval-Schritte**:
   - Embedding-Generierung âœ…
   - Vector-Suche (parallel fÃ¼r Chunks und Chapter-Summaries) âœ…
   - Fetch Neighbors (Window-basiert) âœ…
   - Score-Boosting (Chapter-Boost, Lexical-Boost) âœ…

4. âœ… **Prompt-Building**:
   - TOC-Prompt fÃ¼r TOC-Queries âœ…
   - Normal-Prompt fÃ¼r normale Fragen âœ…

5. âœ… **LLM-Aufruf**:
   - Streaming implementiert âœ…
   - Token-Budget-Management âœ…
   - Retry-Logik bei zu langen Prompts âœ…

6. âœ… **Response-Parsing**:
   - StoryTopicsData fÃ¼r TOC-Queries âœ…
   - Normal-Parsing fÃ¼r normale Fragen âœ…
   - Warnung wird hinzugefÃ¼gt âœ…

### âš ï¸ Verbesserungen identifiziert

1. **Performance**: Fetch Neighbors ist zu langsam (siehe oben)
2. **Monitoring**: Timing-Daten werden gesammelt, aber nicht ausgewertet
3. **Optimierung**: Window-GrÃ¶ÃŸe kÃ¶nnte dynamischer sein

---

## Empfohlene Optimierungen (Priorisiert)

### Sofort umsetzbar (Quick Wins)

1. **Fetch Neighbors batching** (PrioritÃ¤t: Hoch)
   - Batch-GrÃ¶ÃŸe auf 20 IDs reduzieren
   - Parallelisierung implementieren
   - GeschÃ¤tzte Verbesserung: ~50% schneller (2902ms â†’ ~1450ms)

2. **Window-GrÃ¶ÃŸe dynamisch anpassen** (PrioritÃ¤t: Mittel)
   - Kleinere Window-GrÃ¶ÃŸe bei vielen Matches
   - GeschÃ¤tzte Verbesserung: ~30% weniger IDs â†’ ~20% schneller

### Mittelfristig (Wochen)

3. **Top-K dynamisch anpassen** (PrioritÃ¤t: Mittel)
   - Basierend auf Budget und Anzahl der Matches
   - GeschÃ¤tzte Verbesserung: ~10-15% schneller

4. **Caching fÃ¼r Embeddings** (PrioritÃ¤t: Niedrig)
   - Cache fÃ¼r Ã¤hnliche Fragen
   - GeschÃ¤tzte Verbesserung: ~500ms bei Cache-Hit

### Langfristig (Monate)

5. **Monitoring-Dashboard** (PrioritÃ¤t: Niedrig)
   - Performance-Metriken visualisieren
   - Alerts bei langsamen Queries

---

## Zusammenfassung: Performance-Optimierung

### Aktuelle Performance (RAG-Flow)
- **Total Retrieval**: ~4600ms
- **Langsamster Schritt**: Fetch Neighbors (~2902ms, 64%)
- **Zweiter Schritt**: Vector-Suche (~1187ms, 26%)
- **Schnellster Schritt**: Embedding (~514ms, 11%)

### Optimierungspotenzial
- **Fetch Neighbors**: ~50% schneller mÃ¶glich (Batching)
- **Vector-Suche**: ~10-15% schneller mÃ¶glich (dynamisches Top-K)
- **Gesamt**: ~30-40% schneller mÃ¶glich (~4600ms â†’ ~2800-3200ms)

### NÃ¤chste Schritte
1. âœ… Flow validiert - stimmt noch
2. âœ… Performance-Probleme identifiziert
3. âœ… OptimierungsvorschlÃ¤ge dokumentiert
4. âœ… Implementierung der Quick Wins abgeschlossen

---

## Implementierte Optimierungen

### âœ… Fetch Neighbors Batching (Implementiert)

**Problem**: Ein einzelner Request mit vielen IDs (74+) war sehr langsam (~2902ms).

**LÃ¶sung**: 
- IDs werden in Batches aufgeteilt (Standard: 20 IDs pro Batch)
- Batches werden parallel mit `Promise.all()` abgerufen
- Ergebnisse werden zusammengefÃ¼hrt

**Code-Ã„nderungen** (`src/lib/chat/retrievers/chunks.ts`):
```typescript
// Batching: IDs in kleinere Batches aufteilen und parallel abrufen
const BATCH_SIZE = Number(process.env.CHAT_FETCH_BATCH_SIZE) || 20
const batches: string[][] = []
for (let i = 0; i < ids.length; i += BATCH_SIZE) {
  batches.push(ids.slice(i, i + BATCH_SIZE))
}

// Parallel abrufen: Mehrere kleinere Requests sind schneller als ein groÃŸer Request
const fetchedBatches = await Promise.all(
  batches.map(batch => fetchVectors(idx.host, apiKey, batch))
)

// Ergebnisse zusammenfÃ¼hren
const fetched: Record<string, { id: string; metadata?: Record<string, unknown> }> = {}
for (const batch of fetchedBatches) {
  Object.assign(fetched, batch)
}
```

**Konfiguration**:
- Umgebungsvariable: `CHAT_FETCH_BATCH_SIZE` (Standard: 20)
- Kann Ã¼ber `.env` angepasst werden

**Erwartete Verbesserung**: ~50% schneller (2902ms â†’ ~1450ms)

---

### âœ… Dynamische Window-GrÃ¶ÃŸe (Implementiert)

**Problem**: Feste Window-GrÃ¶ÃŸe fÃ¼hrte bei vielen Matches zu sehr vielen IDs.

**LÃ¶sung**:
- Window-GrÃ¶ÃŸe wird dynamisch basierend auf Anzahl der Matches angepasst
- Bei vielen Matches (>30): Window-GrÃ¶ÃŸe 1
- Bei mittleren Matches (>15): Window-GrÃ¶ÃŸe 2
- Sonst: UrsprÃ¼ngliche Window-GrÃ¶ÃŸe basierend auf `answerLength`

**Code-Ã„nderungen** (`src/lib/chat/retrievers/chunks.ts`):
```typescript
// Dynamische Window-GrÃ¶ÃŸe: Kleinere Window-GrÃ¶ÃŸe bei vielen Matches
const baseWindow = input.answerLength === 'ausfÃ¼hrlich' ? 3 : input.answerLength === 'mittel' ? 2 : 1
const dynamicWindow = matches.length > 30 ? 1 : matches.length > 15 ? 2 : baseWindow
const windowByLength = Math.min(dynamicWindow, baseWindow) // Nicht grÃ¶ÃŸer als ursprÃ¼nglich
```

**Erwartete Verbesserung**: ~30% weniger IDs â†’ ~20% schneller

---

### âœ… Dynamisches Top-K (Implementiert)

**Problem**: Festes `baseTopK = 20` war nicht optimal fÃ¼r verschiedene Budgets.

**LÃ¶sung**:
- Top-K wird dynamisch basierend auf verfÃ¼gbarem Budget berechnet
- GrÃ¶ÃŸeres Budget ermÃ¶glicht mehr Chunks

**Code-Ã„nderungen** (`src/lib/chat/retrievers/chunks.ts`):
```typescript
// Dynamisches Top-K basierend auf Budget: GrÃ¶ÃŸeres Budget ermÃ¶glicht mehr Chunks
const baseTopK = budget > 50000 ? 30 : budget > 30000 ? 20 : 15
```

**Logik**:
- Budget > 50000: Top-K = 30
- Budget > 30000: Top-K = 20
- Sonst: Top-K = 15

**Erwartete Verbesserung**: ~10-15% schneller

---

## Performance-Verbesserungen: Zusammenfassung

### Vorher (gemessen)
- **Total Retrieval**: ~4600ms
- **Fetch Neighbors**: ~2902ms (64%)
- **Vector-Suche**: ~1187ms (26%)
- **Embedding**: ~514ms (11%)

### Nachher (geschÃ¤tzt)
- **Total Retrieval**: ~2800-3200ms (~30-40% schneller)
- **Fetch Neighbors**: ~1450ms (~50% schneller durch Batching)
- **Vector-Suche**: ~1000-1070ms (~10-15% schneller durch dynamisches Top-K)
- **Embedding**: ~514ms (unverÃ¤ndert)

### Implementierte Features
1. âœ… Fetch Neighbors Batching mit Parallelisierung
2. âœ… Dynamische Window-GrÃ¶ÃŸe basierend auf Anzahl der Matches
3. âœ… Dynamisches Top-K basierend auf Budget
4. âœ… Konfigurierbare Batch-GrÃ¶ÃŸe Ã¼ber Umgebungsvariable

### Konfiguration

**Neue Umgebungsvariable**:
- `CHAT_FETCH_BATCH_SIZE`: Batch-GrÃ¶ÃŸe fÃ¼r Fetch Neighbors (Standard: 20)
  - Kann in `.env` gesetzt werden
  - Empfohlener Wert: 20-30 (abhÃ¤ngig von MongoDB-Performance)

---

## Monitoring & Testing

### Empfohlene Tests
1. **Unit-Tests**: Batching-Logik mit verschiedenen ID-Anzahlen
2. **Integration-Tests**: Gesamter Retrieval-Flow mit Timing-Messungen
3. **Edge-Cases**: 
   - Leere IDs-Liste
   - Sehr viele IDs (>100)
   - Einzelne ID
   - Batch-GrÃ¶ÃŸe grÃ¶ÃŸer als Anzahl der IDs

### Performance-Monitoring
- Timing-Daten werden bereits in `QueryRetrievalStep` gespeichert
- Empfehlung: Dashboard fÃ¼r Performance-Metriken erstellen
- Alerts bei langsamen Queries (>5s) einrichten

